{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIwkMy9WnrRn"
   },
   "source": [
    "# <center> Introduction to Reinforcement Learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAVU9RrU9rRB"
   },
   "source": [
    "# Getting Used to the Grid World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FY8sb3WLnrT6"
   },
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHhQP_yrnrUA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from IntroRL_Support.helper import *\n",
    "from ece4078.gym_simple_gridworlds.envs.grid_env import GridEnv\n",
    "from ece4078.gym_simple_gridworlds.envs.grid_2dplot import *\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "UP = 0; DOWN = 1; LEFT = 2; RIGHT = 3; STAY = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the grid world we are dealing with:\n",
    "\n",
    "![GridWorldExample.png](https://i.postimg.cc/5tMM5vqf/Grid-World-Example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining a few existing methods in GridEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__get_reachable_states__(s)` given a state `s`, what is the state that the agent might end up in for all possible actions, remember that the grid has borders, and the agent cannot go into obstacle tile.\n",
    "\n",
    "Try to predict the output of the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)\n",
    "grid_world.__get_reachable_states__(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `rewards` property of class `GridEnv` is an $n\\times m$ matrix where $n$ is the size of the state space, and m is the size of the action space. Each entry is the expected reward given the state-action pair. Let's try to manually calcuclate a few of these entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world.rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a new transition function\n",
    "\n",
    "Let's make the environment such that there is an $p$ chance that the agent will move as we expected, and $1 - p$ chance that it will stay exactly where it is. Given $|p| \\leq 1$.\n",
    "\n",
    "Note that obstacle is a state with value `np.nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transition_LUT(env, p):\n",
    "    state_size = env.observation_space.n\n",
    "    action_size = env.action_space.n\n",
    "    state_transitions = np.zeros((state_size, action_size, state_size))\n",
    "\n",
    "    #TODO 1: Specify the new transition function -------------------------------------\n",
    "    \n",
    "    #ENDTODO -------------------------------------------------------------------------\n",
    "    return state_transitions\n",
    "\n",
    "state_transitions = generate_transition_LUT(grid_world, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a new reward function\n",
    "\n",
    "Let's make the reward such that moving UP or DOWN is considered a really bad thing. Make the reward such that whenever you move up or down, the reward is subtracted by `up_down_penalty`. Otherwise the state-action reward is $R(s,a) = \\sum_{s^{\\prime}\\in\\mathcal{S}}\\mathbb{P}(s^{\\prime}| s, a)\\mathcal{R}(s^{\\prime})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_state_action_reward_LUT(env, up_down_penalty):\n",
    "    state_size = env.observation_space.n\n",
    "    action_size = env.action_space.n\n",
    "    rewards = np.zeros((state_size, action_size))\n",
    "\n",
    "    #TODO 2: Specify new reward function --------------------------------------------\n",
    "    \n",
    "    #ENDTODO -------------------------------------------------------------------------\n",
    "    return rewards\n",
    "\n",
    "rewards = generate_state_action_reward_LUT(grid_world, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a stochastic policy\n",
    "\n",
    "Define a policy that is state-independent, i.e. its action is the same regardless of the state it is in. The function expects 4 inputs:\n",
    "- `up_chance`: the probability that the robot will go up\n",
    "- `down_chance`: the probability that the robot will go down\n",
    "- `left_chance`: the probability that the robot will go left\n",
    "- `right_chance`: the probability that the robot will go right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(up_chance, down_chance, left_chance, right_chance):\n",
    "    #TODO 3: Define stochastic policy ------------------------------------------------\n",
    "        pass\n",
    "    #ENDTODO -------------------------------------------------------------------------\n",
    "\n",
    "my_policy = policy(0.5, 0.3, 0.15, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let visualize what happens when we use these reward, policy and transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Grid World instance\n",
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)\n",
    "s_x, s_y = get_state_to_plot(grid_world)\n",
    "fig, ax = grid_world.render()\n",
    "agent, = ax.plot([], [], 'o', color='b', linewidth=6)\n",
    "reward_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n",
    "\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "cur_state = grid_world.cur_state\n",
    "path_to_plot = []\n",
    "\n",
    "grid_world.state_transitions = generate_transition_LUT(grid_world, 0.7)\n",
    "grid_world.rewards = generate_state_action_reward_LUT(grid_world, 0.1)\n",
    "my_policy = lambda: policy(0.5, 0.3, 0.15, 0.05)\n",
    "i = 0\n",
    "max_steps = 1000\n",
    "\n",
    "while not done:\n",
    "    _, cur_reward, done, _ = grid_world.step(int(my_policy()))\n",
    "    i = i + 1\n",
    "    cur_state = grid_world.cur_state\n",
    "    n_x, n_y = get_state_to_plot(grid_world)\n",
    "    cumulative_reward += cur_reward\n",
    "    path_to_plot.append([cumulative_reward, n_x, n_y])\n",
    "    if i > max_steps:\n",
    "        done = True\n",
    "        print(f\"Could not reach terminal state after {max_steps} steps.\")\n",
    "\n",
    "def init():\n",
    "    agent.set_data([s_x + 0.5], [s_y + 0.5])\n",
    "    reward_text.set_text('')\n",
    "    return agent, reward_text\n",
    "\n",
    "def animate(i):\n",
    "    if i < len(path_to_plot):\n",
    "        r, n_x, n_y = path_to_plot[i]\n",
    "        agent.set_data([n_x + 0.5], [n_y + 0.5])\n",
    "        reward_text.set_text('Cumulative reward: %.2f' % r)\n",
    "    return agent, reward_text\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(path_to_plot), blit=False, interval=500, init_func=init,\n",
    "                              repeat=False)\n",
    "\n",
    "plt.close('all') \n",
    "display(HTML(f\"<div align=\\\"center\\\">{ani.to_jshtml()}</div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import otter\n",
    "from ece4078.Utility import pretty_print_otter\n",
    "grader = otter.Notebook(tests_dir = \"IntroRL_Support/tests\")\n",
    "\n",
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IntroRL_Exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
