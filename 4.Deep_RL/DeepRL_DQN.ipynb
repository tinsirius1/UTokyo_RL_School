{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d2bddc9",
   "metadata": {},
   "source": [
    "# Deep RL - Deep Q-Learning (DQN)\n",
    "\n",
    "### Import required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepRL_Support.render import *\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sample_agent = 'DeepRL_Support/DQNagent_sample.pt'\n",
    "\n",
    "def set_seed(env, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    env.reset(seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a8699f",
   "metadata": {},
   "source": [
    "# 1. Lunar Lander Environment\n",
    "\n",
    "In this notebook, we will use DQN to solve the [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) environment.\n",
    "\n",
    "![lunar-lander.gif](https://i.postimg.cc/sxmVCGwC/lunar-lander.gif)\n",
    "\n",
    "## Task\n",
    "Land the lunar lander on the landing pad at coordinates (0,0) in between the flags. You can control the luanr lander with its 3 engines, pointing downward, left, and right respectively.\n",
    "\n",
    "## State Space\n",
    "There are 8 state variables:\n",
    "\n",
    "| State Variable | Description                        |\n",
    "| :------------- | ---------------------------------: |\n",
    "| x              | x coordinate of the lander         | \n",
    "| y              | y coordinate of the lander         | \n",
    "| $v_x$          | horizontal velocity of the lander  | \n",
    "| $v_y$          | vertical velocity of the lander    | \n",
    "| $\\theta$       | the lander's orientation in space  | \n",
    "| $v_\\theta$     | angular velocity of the lander     | \n",
    "| legs[0]        | 1 if left leg has contact, else 0  | \n",
    "| legs[1]        | 1 if right leg has contact, else 0 | \n",
    "\n",
    "## Action Space\n",
    "There are 4 discrete actions (note: there is also a [continuous version](https://gym.openai.com/envs/LunarLanderContinuous-v2/) offered by OpenAI Gym):\n",
    "\n",
    "| Num | Action            | \n",
    "| :-- | ----------------: |\n",
    "| 0   | Do nothing        |\n",
    "| 1   | Fire left engine  |\n",
    "| 2   | Fire down engine  |\n",
    "| 3   | Fire right engine |\n",
    "\n",
    "## Rewards\n",
    "\n",
    "- [Reward](https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py#L362) for moving from the top of the screen to landing pad and rest at zero velocity is about 100 to 140 points. Landing outside landing pad is possible. If lander moves away from landing pad it loses reward back. \n",
    "- Episode finishes if the lander crashes or comes to rest, receiving an additional -100 or +100 points. \n",
    "- Each leg having ground contact is +10. \n",
    "- Firing main engine is -0.3 points each frame. Fuel is infinite. \n",
    "- The environment is considered solved when the average return is greater than or equal to 200 points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8367c6f",
   "metadata": {},
   "source": [
    "# 2. A random agent\n",
    "Here we create an agent that takes random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f70137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "\n",
    "    def __init__(self, n, seed=None):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.n = n\n",
    "\n",
    "    def act(self, observation):\n",
    "        # Sample one action in [0, n).\n",
    "        return self.rng.integers(low=0, high=self.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ceac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment and an agent.\n",
    "env = gym.make(\"LunarLander-v2\", render_mode = \"rgb_array\")\n",
    "\n",
    "agent = RandomAgent(env.action_space.n)\n",
    "\n",
    "# Create animation of simulation.\n",
    "frames, reward = simulate(agent, env, max_frames=500)\n",
    "env.close()\n",
    "print(\"Rendering to Video ...\")\n",
    "html = animate(frames)\n",
    "\n",
    "# Show the animation.\n",
    "display(html)\n",
    "print(f\"Visualizing the random agent. Its reward is {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9b926",
   "metadata": {},
   "source": [
    "# 3. DQN Algorithm\n",
    "\n",
    "Our implementation follows the algorithm proposed by *Mnih et .al* in  <a href=\"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\">Playing Atari with Deep Reinforcement Learning</a> \n",
    "\n",
    "![dqn-replay-1.png](https://i.postimg.cc/6p6QtvbQ/dqn-replay-1.png)\n",
    "\n",
    "We have decomposed the algorithm shown above into:\n",
    "- A ReplayMemory class to represent and encode the Replay Buffer\n",
    "- A DQN class to represent our NN function approximator\n",
    "- An agent class which contains the learning logic of the algorithm\n",
    "- A main loop in which transition tuples ('state', 'action', 'next_state', 'reward', 'done') are generated and added to the replay buffer. This loop also calls the agent ``optimize(.)`` method to train our approximator\n",
    "\n",
    "## 4.1 Replay Buffer\n",
    "\n",
    "Let's first define our representation of the replay buffer. To do so, we will use the class ``ReplayMemory`` shown below\n",
    "\n",
    "**Note**: This implementation of the ReplayMemory class was taken from [***Pytorch DQN tutorial***](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b3e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tuple represents one observation in our environment\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ed895-97a4-4b38-94ac-fa9eaf3bc2bd",
   "metadata": {},
   "source": [
    "## 4.2 DQN Network\n",
    "\n",
    "Let us now define the Multi Layer Perceptron network that will be used as the function approximator for the action-value function (q-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c9be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"DQN Network\n",
    "        Args:\n",
    "        input_dim (int): `state` dimension.\n",
    "        output_dim (int): Number of actions.\n",
    "        hidden_dim (int): Hidden dimension in fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs=8, num_actions=4, hidden_dim_1=32, hidden_dim_2=32):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_dim_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_1, hidden_dim_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_2, num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns a Q_value\n",
    "        Args:\n",
    "            x (torch.Tensor): `State` 2-D tensor of shape (n, num_inputs)\n",
    "        Returns:\n",
    "            torch.Tensor: Q_value, 2-D tensor of shape (n, num_actions)\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a4228",
   "metadata": {},
   "source": [
    "## 4.3 DQN Agent\n",
    "\n",
    "This class contains the main steps of the Deep Q-learnig algorithm (highlighted in blue) in the image shown above.\n",
    "\n",
    "**TODO**: \n",
    "- Complete the computation of the target value. Use the variables ``non_final_mask`` and ``non_final_next_states`` to do so. Similarly, the method ``get_next_q()`` returns the q-value of the maximum valued action at a given state, i.e., $\\max_{a'}Q(s,a';\\theta)$\n",
    "- Compute the loss. That is the difference between the target q-values (``expected_q``) and the values estimated by the network (``predicted_q``). Use the attribute ``self.loss_fn(.)``.\n",
    "\n",
    "Keep in mind how the targets are computed in the original algorithm\n",
    "\n",
    "$$\n",
    "\\text{Set } y_j = \n",
    "\\begin{cases}\n",
    "r_j & \\text{for terminal } \\phi_{j+1} \\\\ \n",
    "r_j + \\gamma \\max_{a^{\\prime}} Q(\\phi_{j+1}, a^{\\prime}; \\theta) & \\text{for non-terminal } \\phi_{j+1}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efcf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    \"\"\"DQN Agent\n",
    "    This class contains the main steps of the DQN algorithm\n",
    "    \n",
    "    Attributes:\n",
    "    q_NN (DQN): Function approximator for our target q function\n",
    "    loss_fn (MSELoss): Criterion that measures the mean squared error (squared L2 norm) \n",
    "                       between each element of the predicted and target q-values.\n",
    "    optimizer (Adam): Stochastic gradient optimizer\n",
    "    gamma (float): Discount factor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=8, output_dim=4, \n",
    "                 hidden_dim_1=32, hidden_dim_2=32, gamma=0.99, lr=0.0001):\n",
    "        \"\"\"\n",
    "        Define instance of DQNAgent\n",
    "        Args:\n",
    "        input_dim (int): `state` dimension.\n",
    "        output_dim (int): Number of actions.\n",
    "        hidden_dim (int): Hidden dimension in fully connected layer\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.q_NN = DQN(input_dim, output_dim, hidden_dim_1, hidden_dim_2).to(device)\n",
    "                \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.q_NN.parameters(), lr=lr)\n",
    "                \n",
    "        self.gamma = torch.tensor(gamma).float().to(device)\n",
    "        \n",
    "    def get_action(self, state, action_space_dim, epsilon):\n",
    "        \"\"\"\n",
    "        Select next action using epsilon-greedy policy\n",
    "        Args:\n",
    "        epsilon (float): Threshold used to decide whether a random or maximum-value action \n",
    "                         should be taken next\n",
    "         Returns:\n",
    "            int: action index\n",
    "        \"\"\"        \n",
    "        with torch.no_grad():\n",
    "            cur_q = self.q_NN(torch.from_numpy(state).float().to(device))\n",
    "        q_value, action = torch.max(cur_q, axis=0)\n",
    "        action = action if torch.rand(1,).item() > epsilon else torch.randint(0, action_space_dim, (1,)).item()\n",
    "        action = torch.tensor([action]).to(device)\n",
    "        return action\n",
    "    \n",
    "    def get_next_q(self, state):\n",
    "        \"\"\"Returns Q_value for maximum valued action at each state s\n",
    "        Args:\n",
    "            x (torch.Tensor): `State` 2-D tensor of shape (n, num_inputs)\n",
    "        Returns:\n",
    "            torch.Tensor: Q_value, 1 tensor of shape (n)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            next_q = self.q_NN(state)\n",
    "        q, _ = torch.max(next_q, axis=1)\n",
    "        return q\n",
    "    \n",
    "    def optimize(self, batch):\n",
    "        \"\"\"Computes `loss` and backpropagation\n",
    "        Args:\n",
    "            batch: List[Transition]: Minibatch of `Transition`\n",
    "        Returns:\n",
    "            float: loss value\n",
    "        \"\"\"\n",
    "        \n",
    "        state_batch = torch.stack(batch.state)\n",
    "        action_batch = torch.stack(batch.action)\n",
    "        reward_batch = torch.stack(batch.reward)\n",
    "        next_state_batch = torch.stack(batch.next_state)\n",
    "                \n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state is the one after which the simulation ends)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s.item() is not True,\n",
    "                                          batch.done)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.stack([s for i, s in enumerate(next_state_batch)\n",
    "                                            if batch.done[i].item() is not True])\n",
    "\n",
    "        # Compute predicted q-values\n",
    "        predicted_q = self.q_NN(state_batch).gather(1, action_batch).reshape(1,-1)\n",
    "        \n",
    "        #TODO 1: Compute expected values for non-terminal and terminal states (this is our TD target) --\n",
    "        target_q = 0\n",
    "        target_q[non_final_mask] = 0\n",
    "        #ENDTODO ---------------------------------------------------------------------------------------\n",
    "        \n",
    "        expected_q = reward_batch.reshape(1,-1)+(self.gamma * target_q)\n",
    "        \n",
    "        #TODO 2: Compute loss --------------------------------------------------------------------------\n",
    "        loss = 0\n",
    "        #ENDTODO ---------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Use loss to compute gradient and update policy parameters through backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.optimizer.step()\n",
    "                \n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35bc9de",
   "metadata": {},
   "source": [
    "### Running Parameters\n",
    "\n",
    "Note that here we are using number of frames to control the duration of the training. An episode contains varied number of frames and ends when the lander crashes or comes to rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define running hyper-parameters and epsilon training sequence\n",
    "memory_capacity = 250\n",
    "batch_size = 64\n",
    "num_frames = 2500\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 400\n",
    "gamma = 0.99\n",
    "lr = 1e-3\n",
    "hidden_dim_1 = 32\n",
    "hidden_dim_2 = 32\n",
    "seed_value = 42\n",
    "\n",
    "epsilon_by_step = lambda frame_idx: epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7554a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "ax.plot([epsilon_by_step(i) for i in range(num_frames)])\n",
    "ax.set_xlabel(\"Num. steps\")\n",
    "ax.set_ylabel(\"Epsilon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cba9c19",
   "metadata": {},
   "source": [
    "### Main Loop and Replay Buffer Control\n",
    "\n",
    "This is the main loop of our DQN implementation. Here we generate the samples added to the replay memory and train the agent using a batch sampled for the replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define varibles for plotting\n",
    "losses_list, rewards_list, episode_len_list = [], [], []\n",
    "list_epsilon = []\n",
    "\n",
    "# Create instance of reply buffer\n",
    "replay_buffer = ReplayMemory(memory_capacity)\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"LunarLander-v2\", render_mode = \"rgb_array\")\n",
    "set_seed(env, seed_value)\n",
    "n_actions = env.action_space.n\n",
    "dim_state = env.observation_space.shape[0]\n",
    "\n",
    "# Create agent\n",
    "agent = DQNAgent(input_dim=dim_state, \n",
    "                 output_dim=n_actions, \n",
    "                 hidden_dim_1=hidden_dim_1, \n",
    "                 hidden_dim_2=hidden_dim_2, \n",
    "                 gamma=gamma, lr=lr)\n",
    "\n",
    "# Reset environment and set all counters and cummulative varibles to zero\n",
    "state, ep_len, losses, episode_reward = env.reset()[0], 0, 0, 0\n",
    "\n",
    "for frame_idx in tqdm(range(1, num_frames + 1)):    \n",
    "    # Get epsilon\n",
    "    cur_epsilon = epsilon_by_step(frame_idx)\n",
    "    \n",
    "    # Sample action using e-greedy policy\n",
    "    action = agent.get_action(state, n_actions, cur_epsilon)\n",
    "    \n",
    "    # Apply action and observe changes in the environment\n",
    "    next_state, reward, done, truncated, _ = env.step(action.item())\n",
    "    episode_reward += reward\n",
    "    \n",
    "    # Transform observation into Transition tuple\n",
    "    t_s = torch.tensor(state).float().to(device)\n",
    "    t_r = torch.tensor([reward]).float().to(device)\n",
    "    t_ns = torch.tensor(next_state).float().to(device)\n",
    "    t_a = action.to(device)\n",
    "    t_done = torch.tensor([done or truncated]).bool().to(device)\n",
    "                \n",
    "    # Add new sample to replay buffer\n",
    "    replay_buffer.push(t_s, t_a, t_ns, t_r, t_done)\n",
    "    state = next_state\n",
    "    \n",
    "    ep_len += 1\n",
    "    \n",
    "    # If current episode has finished, reset environment and counters for next episode\n",
    "    if (done or truncated):\n",
    "        state = env.reset()[0]\n",
    "        rewards_list.append(episode_reward)\n",
    "        episode_len_list.append(ep_len)\n",
    "        episode_reward, ep_len = 0, 0\n",
    "    \n",
    "    # If replay buffer has at least batch_size elements, sample batch and train approximator\n",
    "    if len(replay_buffer) > batch_size:\n",
    "        transitions = replay_buffer.sample(batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        loss = agent.optimize(batch)\n",
    "        losses_list.append(loss)\n",
    "    \n",
    "    # Every 200 steps we plot the approximator's progress and performance\n",
    "    if frame_idx % 200 == 0:\n",
    "        plot(frame_idx, rewards_list, losses_list)\n",
    "\n",
    "# Clear memory by freeing up the replay buffer\n",
    "del replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained agent\n",
    "file_name = 'DQNagent.pt'\n",
    "torch.save(agent, file_name)\n",
    "# file_name = sample_agent # uncomment this line to visualize performance of a sample DQN agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f60be07",
   "metadata": {},
   "source": [
    "### Let's now test our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b0f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "file_name = file_name\n",
    "VISUALIZE_TRIAL_K = 7\n",
    "\n",
    "agent = torch.load(file_name)\n",
    "agent.q_NN.to(device)\n",
    "\n",
    "# run n trials to examine how this agent performs\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "set_seed(env, seed_value)\n",
    "n_trials = 10\n",
    "list_rewards = []\n",
    "\n",
    "wrapagent = DQNWrapper(agent)\n",
    "\n",
    "# test on n trials \n",
    "for i in range(n_trials):\n",
    "\n",
    "    # if you want to see the animation increase max_frames (slower)\n",
    "    if i == VISUALIZE_TRIAL_K:\n",
    "        framebuffer, ep_return = simulate(wrapagent, env, max_frames=700)\n",
    "    else:\n",
    "        _, ep_return = simulate(wrapagent, env, max_frames=1)\n",
    "    env.close()\n",
    "    # Store the returns\n",
    "    list_rewards.append(ep_return)\n",
    "    print(f\"Trial No.{i}, return: {ep_return}\")\n",
    "\n",
    "# summarize the trials\n",
    "print(f'\\nAverage return {np.round(np.mean(list_rewards),3)} +- {np.round(np.std(list_rewards), 3)}')\n",
    "\n",
    "print(\"Rendering to Video ...\")\n",
    "html = animate(framebuffer)\n",
    "display(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
