{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da37cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from ModelFreeRL_Support.helper import *\n",
    "from ModelFreeRL_Support.dp_algorithms import *\n",
    "from ece4078.gym_simple_gridworlds.envs.grid_env import GridEnv\n",
    "from ece4078.gym_simple_gridworlds.envs.grid_2dplot import *\n",
    "\n",
    "from collections import namedtuple, defaultdict\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82a39e",
   "metadata": {},
   "source": [
    "# 1. The Grid World environment\n",
    "\n",
    "Recall the grid in which our robot lives\n",
    "\n",
    "![GridWorldExample.png](https://i.postimg.cc/5tMM5vqf/Grid-World-Example.png)\n",
    "\n",
    "- The states $s \\in \\mathcal{S}$ correspond to locations in the grid. Each location has also a cell index associated to it, e.g., cell index 4 is associated to location (row=1,col=0)\n",
    "- The robot can move up, down, left, or right. Actions correpond to unit increments or decrements in the specified direction.\n",
    "    - Up : (-1,0)\n",
    "    - Down: (1,0)\n",
    "    - Left: (0,-1)\n",
    "    - Right: (0, 1)\n",
    "- Each action is represented by a number. Action (Up) is represented by 0, (Down) by 1, (Left) by 2 and, finally, (Right) by 3. No actions are available at a terminal state\n",
    "- Discount factor $\\gamma = 0.9$ (class attribute ``gamma=0.9``)\n",
    "- Stochastic transition matrix (class attribute ``noise=0.2``)\n",
    "- Rewards are only obtained at terminal states (class attribute ``living_reward=-0.04``)\n",
    "\n",
    "### Known Model\n",
    "\n",
    "Recall also the **optimal policy** we found using policy-iteration\n",
    "\n",
    "![example_policy.png](https://i.postimg.cc/pLjHnkj0/example-policy.png)\n",
    "\n",
    "since the dynamics of our grid world environment are known, we obtained the state-value function $v_\\pi(s)$ associated to this policy using ``policy_evalution(.)`` \n",
    "\n",
    "We have defined the class ``GridEnv`` to represent our Grid World MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d49ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Grid World instance\n",
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)\n",
    "\n",
    "# Get policy shown in image\n",
    "policy_pi = encode_policy(grid_world)\n",
    "\n",
    "# Compute value-function using dynamic programming\n",
    "v_pi = policy_evaluation(grid_world, policy_pi)\n",
    "\n",
    "plot_value_function(grid_world, v_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a0591",
   "metadata": {},
   "source": [
    "# 2. Monte Carlo Methods\n",
    "\n",
    "## 2.1 Monte-Carlo Policy Evaluation\n",
    "\n",
    "### What if the Transition and Reward Function are Unknown?\n",
    "\n",
    "Let's first define the helper method ``generate_episode(.)``. It samples an episode i.e., a sequence of ($s, a, r, s'$) tuples, from a given policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c14f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample = namedtuple('Sample', ['state', 'action', 'reward', 'next_state'])\n",
    "\n",
    "def generate_episode(grid_env, policy):\n",
    "    \"\"\"\n",
    "    Generate an episode of experiences in environment under a given policy\n",
    "    :param grid_env (GridEnv): Environment\n",
    "    :param policy (dict of probabilites): Policy used to sample actions\n",
    "    \n",
    "    :return List(Sample) Complete episode\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "\n",
    "    # Reset the environment to a random initial state\n",
    "    state = grid_env.reset()\n",
    "\n",
    "    # Set flag to indicate whether episode has ended\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Get actions available at current state\n",
    "        all_actions = list(policy_pi[state].keys())\n",
    "        # Get action probabilities\n",
    "        all_probabilities = np.array(list(policy[state].values()))\n",
    "        # Sample an action from policy\n",
    "        action = np.random.choice(all_actions, 1, p=all_probabilities)[0]\n",
    "        \n",
    "        next_state, reward, done, info = grid_env.step(action)\n",
    "        episode.append(Sample(state, action, reward, next_state))\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e7910",
   "metadata": {},
   "source": [
    "Now, under the assumption that $\\mathcal{T}(s,a,s')$ and $\\mathcal{R}(s,a)$ are unknown, let's use the algorithm shown below to get an estimate $\\hat{v}_\\pi(s)$ of the true state-value function $v_\\pi(s)$\n",
    "\n",
    "![MCPolicyEvaluation.png](https://i.postimg.cc/6pXj5P6D/MCPolicy-Evaluation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_first_visit_policy_evaluation(grid_env, policy, true_v, n_episodes=1):\n",
    "    \"\"\"\n",
    "    Compute estimate of state-value function for a given policy\n",
    "    :param grid_env (GridEnv): Environment\n",
    "    :param policy (dict of probabilites): Policy to be evaluated\n",
    "    :param true_v (dict of floats): True state-value function. Used to compute prediciton error\n",
    "    :param n_episodes (int): Number of episodes to use for prediction\n",
    "    \n",
    "    :return List(float): Prediction error after each episode\n",
    "    :return dict(float): Predicted state-value function\n",
    "    \n",
    "    \"\"\"\n",
    "    all_states = grid_env.get_states()\n",
    "    \n",
    "    # Counter of visits for all states\n",
    "    state_visits = {s:0 for s in all_states}\n",
    "    # Cummulative return for each state\n",
    "    state_returns = {s:0 for s in all_states}\n",
    "    # Predicted state-value function\n",
    "    pred_v = {s:0 for s in all_states}\n",
    "    \n",
    "    # Variable used for plotting\n",
    "    list_errors = []\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        # Generate episode\n",
    "        episode = generate_episode(grid_env, policy)\n",
    "        # Create auxiliary variable to keep of first state visits\n",
    "        visited = {s: False for s in all_states}\n",
    "        # Return for current episode\n",
    "        g = 0\n",
    "        # Variable used to keep track of prediction error for this episode\n",
    "        error = 0\n",
    "        # Starting from last sampled observation in episode\n",
    "        for obs in episode[::-1]:\n",
    "            # Get visite state\n",
    "            s = obs.state\n",
    "            # Compute return for this state\n",
    "            g = g*grid_env.gamma + obs.reward    \n",
    "            # If this is the first time visiting this state\n",
    "            if not visited[s]:\n",
    "                # Increment the visit counter\n",
    "                state_visits[s] += 1\n",
    "                # Add return\n",
    "                state_returns[s] += g\n",
    "                # Compute mean return\n",
    "                pred_v[s] = state_returns[s]/float(state_visits[s])\n",
    "                # Set the state as visited\n",
    "                visited[s] = True\n",
    "                # Compute error\n",
    "                error += np.abs(true_v[s] - pred_v[s])\n",
    "        \n",
    "        list_errors.append(error)\n",
    "    \n",
    "    return list_errors, pred_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62400e18",
   "metadata": {},
   "source": [
    "Let's now try the algorithm and compare its out to the true value-state function. \n",
    "\n",
    "**Interaction**:\n",
    "Run the algorithm multiple times and observe what happens when the number of episodes increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeccb859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change n_episodes to see what happens\n",
    "errors, predicted_v = monte_carlo_first_visit_policy_evaluation(grid_world, policy_pi, v_pi, n_episodes=100)\n",
    "\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "spec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\n",
    "f_ax1 = fig.add_subplot(spec[0, 0])\n",
    "f_ax2 = fig.add_subplot(spec[0, 1])\n",
    "f_ax3 = fig.add_subplot(spec[1, :])\n",
    "\n",
    "#Plot true value function\n",
    "plot_value_function(grid_world, v_pi, f_ax1)\n",
    "f_ax1.set_title(\"True state-value function\")\n",
    "\n",
    "plot_value_function(grid_world, predicted_v, f_ax2)\n",
    "f_ax2.set_title(\"Predicted state-value function\")\n",
    "\n",
    "f_ax3.plot(errors)\n",
    "f_ax3.set_title(\"Predicted Error (sum of abs. differences)\")\n",
    "f_ax3.set_xlabel(\"Num. episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6b316",
   "metadata": {},
   "source": [
    "In a model-free setting where our state-value and action-value estimates depend on the actions chosen by the agent, how can we guarantee that the all actions will continue to be selected?\n",
    "\n",
    "## 2.2 $\\epsilon$-Greedy Policies\n",
    "\n",
    "We can use an $\\epsilon$-greedy policy. This type of policy are formally defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    \\pi(a|s) = \n",
    "    \\begin{cases}\n",
    "        1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}|},&  \\text{if } a^* = \\arg\\max_{a \\in \\mathcal{A}} q_\\pi(s,a)\\\\\n",
    "        \\frac{\\epsilon}{|\\mathcal{A}|}, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Let's see how the agent behaves when it follows an $\\epsilon$-greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_egreedy_action(grid_env, state, q_value, epsilon):\n",
    "    \"\"\"\n",
    "    Select action to execute at a given state under an epsilon-greedy policy\n",
    "    :param grid_env (GridEnv): Grid world environment\n",
    "    :param state (int): Location in grid for which next action is going to be choosen\n",
    "    :param q_value (dict): Action-value function \n",
    "    :param epsilon (float): Randomness threshold used to choose action\n",
    "    \"\"\"\n",
    "    \n",
    "    rand_n = np.random.random()\n",
    "    if rand_n <= epsilon:\n",
    "        return grid_env.action_space.sample()\n",
    "    else:\n",
    "        actions = list(q_value[state].keys())\n",
    "        return actions[np.argmax(list(q_value[state].values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d32e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set noise to zero. Randomness in agent behaviour is only due to e-greedy policy\n",
    "grid_world = GridEnv(noise=0, living_reward=-0.04, gamma=0.9)\n",
    "\n",
    "# Get policy shown in section 1\n",
    "policy_pi = encode_policy(grid_world)\n",
    "\n",
    "# Compute value-function using dynamic programming\n",
    "v_pi = policy_evaluation(grid_world, policy_pi)\n",
    "\n",
    "# Use value-function to compute q-values\n",
    "q_pi = grid_world.get_q_values(v_pi)\n",
    "\n",
    "# Start episode\n",
    "cur_state = grid_world.idx_cur_state\n",
    "s_x, s_y = get_state_to_plot(grid_world)\n",
    "\n",
    "# We can visualize our grid world using the render() function\n",
    "fig, ax = grid_world.render()\n",
    "agent, = ax.plot([], [], 'o', color='b', linewidth=6)\n",
    "reward_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n",
    "\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "path_to_plot = []\n",
    "\n",
    "v_epsilon = 0.8\n",
    "\n",
    "while not done:\n",
    "    action = get_egreedy_action(grid_world, cur_state, q_pi, v_epsilon)\n",
    "    cur_state, cur_reward, done, _ = grid_world.step(int(action))\n",
    "    n_x, n_y = get_state_to_plot(grid_world)\n",
    "    cumulative_reward += cur_reward\n",
    "    path_to_plot.append([cumulative_reward, n_x, n_y])\n",
    "\n",
    "def init():\n",
    "    agent.set_data([s_x + 0.5], [s_y + 0.5])\n",
    "    reward_text.set_text('')\n",
    "    return agent, reward_text\n",
    "\n",
    "def animate(i):\n",
    "    if i < len(path_to_plot):\n",
    "        r, n_x, n_y = path_to_plot[i]\n",
    "        agent.set_data([n_x + 0.5], [n_y + 0.5])\n",
    "        reward_text.set_text('Cumulative reward: %.2f' % r)\n",
    "    return agent, reward_text\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(path_to_plot), blit=False, interval=500, init_func=init,\n",
    "                              repeat=False)\n",
    "\n",
    "plt.close('all') \n",
    "display(HTML(f\"<div align=\\\"center\\\">{ani.to_jshtml()}</div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3bc0cf",
   "metadata": {},
   "source": [
    "# 3. Temporal Difference Methods\n",
    "\n",
    "# 3.1 TD Policy Evaluation\n",
    "Estimate $\\hat{v}_\\pi(s)$ of the true state-value function $v_\\pi(s)$\n",
    "\n",
    "![TDPolicyEvaluation.png](https://i.postimg.cc/c4yywX4c/TDPolicy-Evaluation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c43203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_learning_policy_evaluation(grid_env, policy, true_v, alpha=0.1, n_episodes=1):\n",
    "    \"\"\"\n",
    "    Compute estimate of state-value function for a given policy\n",
    "    :param grid_env (GridEnv): Environment\n",
    "    :param policy (dict of probabilites): Policy to be evaluated\n",
    "    :param true_v (dict of floats): True state-value function. Used to compute prediciton error\n",
    "    :param alpha (float): step-size\n",
    "    :param n_episodes (int): Number of episodes to use for prediction\n",
    "    \n",
    "    :return List(float): Prediction error after each episode\n",
    "    :return dict(float): Predicted state-value function\n",
    "    \n",
    "    \"\"\"\n",
    "    all_states = grid_env.get_states()\n",
    "    \n",
    "    # Predicted state-value function\n",
    "    pred_v = {s:0 for s in all_states}\n",
    "    \n",
    "    # Variable used for plotting\n",
    "    list_errors = []\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        # Generate episode\n",
    "        episode = generate_episode(grid_env, policy)\n",
    "        \n",
    "        # Variable used to keep track of prediction error for this episode\n",
    "        error = 0\n",
    "        \n",
    "        # Starting from the first sampled observation in episode\n",
    "        for obs in episode:\n",
    "            pred_v[obs.state] += alpha * (obs.reward + grid_env.gamma * pred_v[obs.next_state] - pred_v[obs.state]) \n",
    "            error += np.abs(true_v[obs.state] - pred_v[obs.state])       \n",
    "        \n",
    "        list_errors.append(error)\n",
    "    \n",
    "    return list_errors, pred_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b2ca9d",
   "metadata": {},
   "source": [
    "Let's now try the algorithm and compare its output to the true value-state function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaab1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)\n",
    "policy_pi = encode_policy(grid_world)\n",
    "v_pi = policy_evaluation(grid_world, policy_pi)\n",
    "\n",
    "#change n_episodes to see what happens\n",
    "errors, predicted_v = temporal_learning_policy_evaluation(grid_world, policy_pi, v_pi, alpha=0.1, n_episodes=200)\n",
    "\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "spec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\n",
    "f_ax1 = fig.add_subplot(spec[0, 0])\n",
    "f_ax2 = fig.add_subplot(spec[0, 1])\n",
    "f_ax3 = fig.add_subplot(spec[1, :])\n",
    "\n",
    "#Plot true value function\n",
    "plot_value_function(grid_world, v_pi, f_ax1)\n",
    "f_ax1.set_title(\"True state-value function\")\n",
    "\n",
    "plot_value_function(grid_world, predicted_v, f_ax2)\n",
    "f_ax2.set_title(\"Predicted state-value function\")\n",
    "\n",
    "f_ax3.plot(errors)\n",
    "f_ax3.set_title(\"Predicted Error (sum of abs. differences)\")\n",
    "f_ax3.set_xlabel(\"Num. episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d73de6",
   "metadata": {},
   "source": [
    "## 3.2 Q-Learning\n",
    "\n",
    "We have seen how to evaluate a policy without a model. Let's now find an *approximately* optimal policy using the off-policy control method Q-learning.\n",
    "\n",
    "To help during the learning, we have added a lambda function that iteratively decreases epsilon. Our agent will strongly explore the environment at first to then swicth into exploitation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2325db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_epsilon=0.001\n",
    "max_epsilon=1.0\n",
    "epsilon_decay = 80.0\n",
    "epsilon_by_episode = lambda ep_idx: (min_epsilon \n",
    "         + (max_epsilon - min_epsilon) \n",
    "         * math.exp (-1 * ep_idx/epsilon_decay))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "ax.plot([epsilon_by_episode(i) for i in range(500)])\n",
    "ax.set_xlabel(\"Num. episodes\")\n",
    "ax.set_ylabel(\"Epsilon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53669a11",
   "metadata": {},
   "source": [
    "Here is our implementation of the q-learning algorithm shown below\n",
    "\n",
    "![q-learning.png](https://i.postimg.cc/8z70Yv5C/q-learning.png)\n",
    "\n",
    "**Complete the missing steps**:\n",
    "- Choose an action using an $\\epsilon$-greedy policy (use the function ``get_egreedy_action(.)`` we tested in Section 3.1)\n",
    "- Update our q-function using a greedy (max) policy (use ``q_function[cur_state][action]`` to index our q-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164985a6",
   "metadata": {},
   "source": [
    "**Keep in Mind**: Correspondance between the mathematical notation and implemented code\n",
    "\n",
    "|                 |                            |                  |\n",
    "| :-------------- | -------------------------: | ---------------: |\n",
    "|                 | **Variable/Attribute**     | **Type**         | \n",
    "| $\\epsilon$      | `epsilon_by_episode`       | `float`          |\n",
    "| $\\alpha$        | `alpha`                    | `float`          | \n",
    "| $\\gamma$        | `grid_world.gamma`         | `float`          | \n",
    "| $\\hat{q}(s, a)$ | `q_function[idx_s][idx_a]` | `dict` of `dict` | \n",
    "| $s$             | `cur_state`                | `int`            | \n",
    "| $r$             | `reward`                   | `int`            |\n",
    "| $s^{\\prime}$    | `next_state`               | `int`            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c4053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(grid_env, alpha=0.1, min_epsilon=0.01, max_epsilon=1.0, \n",
    "               epsilon_decay = 80.0, n_episodes=500):\n",
    "    \"\"\"\n",
    "    This function computes an approximately optimal policy using q-learning\n",
    "    \n",
    "    :param grid_env (GridEnv): MDP environment\n",
    "    :param alpha (float): step-size\n",
    "    :param epsilon (float): value used during e-greedy action selection\n",
    "    :return: (dict) State-values for all non-terminal states\n",
    "    \"\"\"\n",
    "        \n",
    "    # This lambda function iteratively decreases epsilon\n",
    "    epsilon_by_episode = lambda ep_idx: min_epsilon + (max_epsilon - min_epsilon) * math.exp (-1 * ep_idx/epsilon_decay)\n",
    "    \n",
    "    # Obtain list of all states in environment\n",
    "    states = grid_env.get_states()\n",
    "    actions = grid_env.get_actions()\n",
    "    q_function = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    # Initialize q_function arbitrarily\n",
    "    for s in states:\n",
    "        for a in actions:\n",
    "            q_function[s][a] = 0\n",
    "    \n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        cur_state = grid_env.reset()\n",
    "        done = False\n",
    "        epsilon = epsilon_by_episode(i_episode)\n",
    "        \n",
    "        while not done:\n",
    "            #TODO 1: Complete off-policy action selection (e-greedy)-----------\n",
    "            action = None\n",
    "            #ENDTODO ----------------------------------------------------------\n",
    "            \n",
    "            next_state, reward, done,_ = grid_env.step(action)\n",
    "            q_next_state = list(q_function[next_state].values())\n",
    "            \n",
    "            #TODO 2: Complete update of q-function -----------------------------\n",
    "            q_function[cur_state][action] += 0\n",
    "            #ENDTODO ------------------------------------------------------------\n",
    "            \n",
    "            cur_state=next_state\n",
    "    \n",
    "    return decode_policy(grid_env, q_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a4a31d",
   "metadata": {},
   "source": [
    "Let's now test our implementation and compare our free-model policy with the one we obtained in the last lecture using value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017505fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)\n",
    "policy_pi = encode_policy(grid_world)\n",
    "v_pi = policy_evaluation(grid_world, policy_pi)\n",
    "\n",
    "q_learning_policy = q_learning(grid_world)\n",
    "\n",
    "# DISPLAYING COMPARISON PLOT\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8), constrained_layout=True)\n",
    "spec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\n",
    "f_ax1 = fig.add_subplot(spec[0, 0])\n",
    "f_ax2 = fig.add_subplot(spec[0, 1])\n",
    "f_ax3 = fig.add_subplot(spec[1, 0])\n",
    "f_ax4 = fig.add_subplot(spec[1, 1])\n",
    "\n",
    "#Plot policy obtained using value-iteration value function\n",
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)\n",
    "value_function, optimal_policy = value_iteration(grid_world)\n",
    "\n",
    "plot_policy(grid_world, optimal_policy, f_ax1)\n",
    "f_ax1.set_title(\"Policy - Value Iteration\")\n",
    "\n",
    "plot_policy(grid_world, q_learning_policy, f_ax2)\n",
    "f_ax2.set_title(\"Policy - Q-learning\")\n",
    "\n",
    "# Compute value function for q_learning policy\n",
    "q_policy_state_values = policy_evaluation(grid_world, encode_policy(grid_world, q_learning_policy))\n",
    "\n",
    "plot_value_function(grid_world, value_function, f_ax3)\n",
    "f_ax3.set_title(\"Value Function - Value Iteration\")\n",
    "\n",
    "plot_value_function(grid_world, q_policy_state_values, f_ax4)\n",
    "f_ax4.set_title(\"Value Function - Q-learning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
