{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Free RL - Q Learning with Continuous State Space\n",
    "\n",
    "### Import required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import gym\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "from ModelFreeRL_Support.helper import animate\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pendulum swing up\n",
    "\n",
    "<div>\n",
    "<img src=\"https://www.gymlibrary.dev/_images/pendulum.gif\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "Please visit [THIS LINK](https://www.gymlibrary.dev/environments/classic_control/pendulum/) for more information about the environment.\n",
    "\n",
    "A quick summary on how to use the environment:\n",
    "\n",
    "```\n",
    "env = gym.make('Pendulum-v1')\n",
    "current_state, _ = env.reset()\n",
    "next_state, reward, done, truncated, info = env.step(action)\n",
    "```\n",
    "\n",
    "where `env.reset()` will reset the environment and initialize it with a random state. `env.step` will move the simulation by one time step apply the action that you passed into it. The return of this function is the state in the next time step `next_state`, the immediate `reward` for being in `current_state` and take action `action`. In the case of Pendulum environment:\n",
    "- `done` is when the agent reached the goal, this always returns False,\n",
    "- `truncated` returns True when the episode end because it ran too long, the default is 200 time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to implement discretization of a continuous state space. In this notebook, I will introduce on of the methods: `np.linspace` and `np.digitize`. \n",
    "\n",
    "Let's say we want to discretize the state space from (-10, 10), and we want 9 different bins (10 dividers). We can generate these dividers with `np.linspace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-10, 10, 10)\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to have a function that receives a number and decide which bin to put it in, that's `np.digitize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.digitize(3.21, bins)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what boundary does this index correspond to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this sequence again with the boundary of the range, and see the behaviour and predict what might go wrong with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins[np.digitize(-10, bins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins[np.digitize(10, bins)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning on Continuous State Space Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Q-table is a 4D matrix, where the first 3 dimensions are the x, y and velocity according to the `gym` documentation. \n",
    "\n",
    "Hence, given a continuous state, we have to obtain the corresponding index of our bins. \n",
    "\n",
    "**TODO**: Let's write a function that receives 4 argument: \n",
    "- `state`: numpy array of 3 elements\n",
    "- `x_bins`: numpy array \n",
    "- `y_bins`: numpy array\n",
    "- `v_bins`: numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state, x_bins, y_bins, v_bins):\n",
    "    #TODO 1: Getting the index corresponding to our bins  ========\n",
    "    pass\n",
    "    #ENDTODO =====================================================\n",
    "\n",
    "# Have a quick test\n",
    "print(\"We expect to see 3 intergers to be printed out such as [12, 35, 56] or (12, 35, 56)\")\n",
    "get_discrete_state([-1.0, 0.5, 1.0],\n",
    "                    np.linspace(-1, 1, 10),\n",
    "                    np.linspace(-1, 1, 10),\n",
    "                    np.linspace(-1, 1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define all of our parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_E=0.001\n",
    "MAX_E=1.0\n",
    "E_DECAY = 80.0\n",
    "NUM_EPISODES = 500\n",
    "epsilon_by_episode = lambda ep_idx: (MIN_E + (MAX_E - MIN_E) * np.exp (-1 * ep_idx/E_DECAY))\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.plot([epsilon_by_episode(i) for i in range(NUM_EPISODES)])\n",
    "ax.set_xlabel(\"Num. episodes\")\n",
    "ax.set_ylabel(\"Epsilon\")\n",
    "\n",
    "NUM_X_BINS = 10\n",
    "NUM_Y_BINS = 10\n",
    "NUM_V_BINS = 10\n",
    "NUM_A_BINS = 10\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Q-learning loop\n",
    "\n",
    "There are 2 TODOs in this cell:\n",
    "- **TODO 2**: Implement the epsilon greedy policy, remember that:\n",
    "    - our `q_table` is a 4D numpy matrix, the first 3 index are for the states (End-Effector X, End-Effector Y, and angular velocity). To get a row of Q-value given a state tuple (x, y, v), we can call `q_table[x, y, v]`\n",
    "    - Apply that action to the environment. Note that the environment accept continous torque in range (-2.0, 2.0) that's why if you find the optimal *discrete* action using `argmax`, please use `action_map` to map that to continous action \n",
    "    - To get the epsilon of current episode, please use function `epsilon_lambda(episode)`\n",
    "- **TODO 3**: Update the Q_table, this should be very similar to the implementation in `ModelFreeRL.ipynb`. The only difference is we are dealing with 3D discrete state instead of 1D discrete state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, epsilon_lambda,\n",
    "               n_episodes, gamma, alpha,\n",
    "               num_x_bins, num_y_bins, num_v_bins, num_a_bins, \n",
    "               saveto = None):\n",
    "    \"\"\"\n",
    "    Q-learning algorithm for the Pendulum environment with a 4D Q-table.\n",
    "    \"\"\"\n",
    "    # Discretize the state and action spaces\n",
    "    x_bins = np.linspace(-1, 1, num_x_bins)\n",
    "    y_bins = np.linspace(-1, 1, num_y_bins)\n",
    "    v_bins = np.linspace(-env.max_speed, env.max_speed, num_v_bins)\n",
    "    action_map = np.linspace(-2.0, 2.0, num_a_bins)\n",
    "\n",
    "    # Initialize Q-table\n",
    "    q_table = np.zeros((num_x_bins, num_y_bins, num_v_bins, num_a_bins))\n",
    "\n",
    "    episode_rewards = []\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        done = False\n",
    "        truncated = False\n",
    "        current_state, _ = env.reset()\n",
    "        Dstate = get_discrete_state(current_state, x_bins, y_bins, v_bins)\n",
    "        cumulative_rewards = 0\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            #TODO 2a: e-greed policy ====================================================\n",
    "            action =\n",
    "            #ENDTODO ====================================================================\n",
    "\n",
    "            #TODO 2b: Take action and observe the state transition and reward ===========\n",
    "            next_state, reward, done, truncated, _ = \n",
    "            #ENDTODO ====================================================================\n",
    "            next_Dstate = get_discrete_state(next_state, x_bins, y_bins, v_bins)\n",
    "\n",
    "            # Update Q-value\n",
    "            #TODO 3: Update the Q table =================================================\n",
    "            \n",
    "            #ENDTODO ====================================================================\n",
    "\n",
    "            #Booking keeping\n",
    "            Dstate = next_Dstate\n",
    "            cumulative_rewards += reward\n",
    "        \n",
    "        episode_rewards.append(cumulative_rewards)\n",
    "\n",
    "    if saveto is not None:\n",
    "        with open(f\"{saveto}.pk\", \"wb\") as f:\n",
    "            pickle.dump({\"q_table\": q_table, \n",
    "                         \"x_bins\": x_bins, \n",
    "                         \"y_bins\": y_bins, \n",
    "                         \"v_bins\": v_bins, \n",
    "                         \"action_map\": action_map}, f)\n",
    "\n",
    "    return q_table, episode_rewards, x_bins, y_bins, v_bins, action_map\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "# Train the agent\n",
    "q_table, e_rewards, x_bins, y_bins, v_bins, action_map = q_learning(env, epsilon_by_episode, NUM_EPISODES, GAMMA, ALPHA,\n",
    "                                                                    NUM_X_BINS, NUM_Y_BINS, NUM_V_BINS, NUM_A_BINS)\n",
    "\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize the policy that you obtained from Q-learning \n",
    "\n",
    "The last TODO is very similar to **TODO 3**, instead of using $\\epsilon$-greedy policy, we use GREEDY policy, that means we always take optimal action according to our q_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelFreeRL_Support.helper import animate\n",
    "\n",
    "frame_buffer = []\n",
    "# Reset to random state\n",
    "state, _ = env.reset(seed = 42)\n",
    "\n",
    "# Initialization for rendering\n",
    "frame = env.render()\n",
    "frame_buffer.append(frame)\n",
    "# Number of steps you want to simulate/render\n",
    "num_steps = 200\n",
    "\n",
    "rewards = 0\n",
    "for _ in range(num_steps):\n",
    "    Dstate = get_discrete_state(state, x_bins, y_bins, v_bins)\n",
    "    #TODO 4: Agent always follow optimal_policy ==============================\n",
    "    action =\n",
    "    next_state, reward, done, truncated, _ = \n",
    "    #ENDTODO ==================================================================\n",
    "    rewards += reward\n",
    "    \n",
    "    # Render to IPython display\n",
    "    frame = env.render()\n",
    "    frame_buffer.append(frame)\n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Cumulative rewards of run: {rewards}\")\n",
    "env.close()\n",
    "print(\"Animating ...\")\n",
    "animate(frame_buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
