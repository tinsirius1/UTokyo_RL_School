{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIwkMy9WnrRn"
   },
   "source": [
    "# <center> Reinforcement Learning Fundamentals</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAVU9RrU9rRB"
   },
   "source": [
    "# Implement Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FY8sb3WLnrT6"
   },
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHhQP_yrnrUA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from RLF_Support.helper import *\n",
    "from ece4078.gym_simple_gridworlds.envs.grid_env import GridEnv\n",
    "from ece4078.gym_simple_gridworlds.envs.grid_2dplot import *\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iY37-IQ9rRC"
   },
   "source": [
    "During the lecture you implemented policy iteration for finding the optimal policy. \n",
    "\n",
    "In this exercise, you are tasked with implementing the value iteration algorithm shown below:\n",
    "    \n",
    "![ValueIteration.png](https://i.postimg.cc/RFg5zYr0/Value-Iteration.png)\n",
    "\n",
    "Please keep in mind:\n",
    "- **TODO 1**: Implement ``one_step_look_ahead(.)`` to calculate $ \\max_{a \\in \\mathcal{A}} \\Bigl(\\mathcal{R}(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}}\\mathcal{T}(s,a,s')v(s')\\Bigr)$. In other words, calculate $\\max_{a\\in\\mathcal{A}}\\mathcal{Q}$, given $\\mathcal{Q}=\\{q(a) | a \\in\\mathcal{A}\\}$, and $q(a) = \\mathtt{q\\_a} = \\mathcal{R}(s,a) + \\gamma\\sum_{s^\\prime\\in\\mathcal{S}}\\mathcal{T}(s, a, s^\\prime)v_{\\pi}(s^{\\prime})$\n",
    "- **TODO 2**: Implemnent ``update_policy(.)`` to obtain the deterministic optimal policy $\\pi^*$ associated to your $v^*(s)$ estimate\n",
    "- **TODO 3**: Implement ``value_iteration(.)`` (which might use functions implemented in TODO 1 and TODO 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_look_ahead(grid_env, s, value_function_):\n",
    "    Q = []\n",
    "    #TODO 1: Examining the policy_iteration code might give you some hint in implementing this function ===================================\n",
    "    max_Q = 0\n",
    "    #ENDTODO ===============================================================================================================================\n",
    "\n",
    "    return max_Q, Q\n",
    "\n",
    "def update_policy(grid_env, value_function_):\n",
    "    non_terminal_states = grid_env.get_states(exclude_terminal=True)\n",
    "    new_policy = np.full(grid_env.grid.shape, np.nan)\n",
    "    #TODO 2: one_step_look_ahead function might make this function shorter ================================================================\n",
    "    \n",
    "    #ENDTODO ================================================================================================================================\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tube_989rRH"
   },
   "outputs": [],
   "source": [
    "def value_iteration(grid_env, threshold=0.00001, plot=False):\n",
    "    \"\"\"\n",
    "    This function iteratively computes optimal state-value function for a given environment grid_env. \n",
    "    It returns the optimal state-value function and its associated optimal policy\n",
    "    \n",
    "    :param grid_env (GridEnv): MDP environment\n",
    "    :param threshoold (float): Convergence threshold\n",
    "    :param plot (bool): Bool argument indicating if value function and policy should be displayed \n",
    "    :return: (tuple) Optimal state-value funciton (dict) and deterministic optimal policy (matrix)\n",
    "    \"\"\"\n",
    "    \n",
    "    #1. Get list of states in environment\n",
    "    states = grid_env.get_states()\n",
    "    \n",
    "    #2. Set convergence threshold and error variable\n",
    "    theta = threshold\n",
    "    delta = 1000\n",
    "\n",
    "    #2. Initialize v function\n",
    "    v = {s: 0.0 for s in grid_env.get_states()}\n",
    "    \n",
    "    #4. Update v(s) until convergence\n",
    "    while delta > theta:\n",
    "        #TODO 3: Use the helper method one_step_look_ahead(.) to update of your current v(s) ------\n",
    "        pass\n",
    "        #ENDTODO ----------------------------------------------------------------------------------\n",
    "            \n",
    "    \n",
    "    #TODO 3: Use the helper method update_policy(.) to obtain the deterministic optimal policy ----\n",
    "    optimal_policy = 0\n",
    "    #ENDTODO --------------------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    if plot:\n",
    "        plot_value_function(grid_env, v)\n",
    "        plot_policy(grid_env, optimal_policy)\n",
    "        \n",
    "    return v, optimal_policy    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_a0_cIV89rRK"
   },
   "source": [
    "### Test your implementation\n",
    "\n",
    "Given an grid world environment with the following attributes:\n",
    "- Discount factor $\\gamma = 0.9$ (class attribute ``gamma=0.9``)\n",
    "- Stochastic transition matrix (class attribute ``noise=0.2``)\n",
    "- Rewards are only obtained at terminal states (class attribute ``living_reward=0.0``)\n",
    "\n",
    "Note: when grading several test cases with different grid world atrributes will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fsIq04s9rRK"
   },
   "outputs": [],
   "source": [
    "# Create a grid world mdp\n",
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=0)\n",
    "\n",
    "optimal_state_function, optimal_policy = value_iteration(grid_world, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvlQp2GuxoKs"
   },
   "source": [
    "Compare your state-value function and optimal policy to the values provided in the test file ``Support/data/ValueIteration_TestCase.pk``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3RMxE-pxfE-"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "ckp_test= 'RLF_Support/pickle/ValueIteration_TestCase.pk'\n",
    "\n",
    "with open(ckp_test, 'rb') as read_from:\n",
    "    test_values = pickle.load(read_from)\n",
    " \n",
    "test_v = test_values['value_state_function']\n",
    "test_p = test_values['optimal_policy']\n",
    "\n",
    "plot_value_function(grid_world, test_v)\n",
    "plot_policy(grid_world, test_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "\n",
    "Expected return:\n",
    "- Your implementation should return the state-value function (in python dictionary) and the optimal policy (matrix form)\n",
    "\n",
    "For grading:\n",
    "- You will be graded based on the output of the ``value_iteration(.)`` method. Please make sure that your solution returns the expected variables with the correct type (no rounding is needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import otter\n",
    "grader = otter.Notebook(tests_dir = \"RLF_Support/tests\")\n",
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RLF_Exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
